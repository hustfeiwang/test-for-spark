rm: cannot remove '/home/netease/test-for-spark/results/examples-core-success.list': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/DFSReadWriteTest': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/myModelPath': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/target': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `/tmp/kmeans': No such file or directory
hdfs://netease:9000/user/root
./run-examples-core.sh: line 22: hdfs://netease:9000/user/root/DFSReadWriteTest: No such file or directory
18/09/09 13:06:09 INFO spark.SparkContext: Running Spark version 2.1.2
18/09/09 13:06:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/09/09 13:06:10 WARN util.Utils: Your hostname, netease resolves to a loopback address: 127.0.1.1; using 10.240.193.109 instead (on interface enp1s0)
18/09/09 13:06:10 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/09/09 13:06:10 INFO spark.SecurityManager: Changing view acls to: root
18/09/09 13:06:10 INFO spark.SecurityManager: Changing modify acls to: root
18/09/09 13:06:10 INFO spark.SecurityManager: Changing view acls groups to: 
18/09/09 13:06:10 INFO spark.SecurityManager: Changing modify acls groups to: 
18/09/09 13:06:10 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
18/09/09 13:06:10 INFO util.Utils: Successfully started service 'sparkDriver' on port 35353.
18/09/09 13:06:10 INFO spark.SparkEnv: Registering MapOutputTracker
18/09/09 13:06:10 INFO spark.SparkEnv: Registering BlockManagerMaster
18/09/09 13:06:10 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/09/09 13:06:10 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/09/09 13:06:10 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a5f12551-eb91-4333-a92c-002d7415037f
18/09/09 13:06:10 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
18/09/09 13:06:10 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/09/09 13:06:10 INFO util.log: Logging initialized @955ms
18/09/09 13:06:10 INFO server.Server: jetty-9.2.z-SNAPSHOT
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/jobs,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/jobs/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/jobs/job,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/jobs/job/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/stages,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/stages/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/stages/stage,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/stages/stage/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/stages/pool,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/stages/pool/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/storage,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/storage/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/storage/rdd,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/storage/rdd/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/environment,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/environment/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/executors,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/executors/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@770d0ea6{/executors/threadDump,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48c40605{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54107f42{/NetEase Executors,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b11ef33{/NetEase Executors/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@476aac9{/NetEase Executors/ne-threadDump,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cea706c{/NetEase Executors/ne-threadDump/json,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bd7f8dc{/static,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f2bf0e2{/,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1eba372c{/api,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ec5d87{/jobs/job/kill,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f9407e{/stages/stage/kill,null,AVAILABLE,@Spark}
18/09/09 13:06:10 INFO server.ServerConnector: Started Spark@3dd69f5a{HTTP/1.1}{0.0.0.0:4040}
18/09/09 13:06:10 INFO server.Server: Started @1032ms
18/09/09 13:06:10 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/09/09 13:06:10 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.193.109:4040
18/09/09 13:06:10 INFO spark.SparkContext: Added JAR file:/root/todo/spark-1.1.1/examples/jars/scopt_2.11-3.3.0.jar at spark://10.240.193.109:35353/jars/scopt_2.11-3.3.0.jar with timestamp 1536469570587
18/09/09 13:06:10 INFO spark.SparkContext: Added JAR file:/root/todo/spark-1.1.1/examples/jars/spark-examples_2.11-2.1.2.jar at spark://10.240.193.109:35353/jars/spark-examples_2.11-2.1.2.jar with timestamp 1536469570588
18/09/09 13:06:10 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://netease:7077...
18/09/09 13:06:10 INFO client.TransportClientFactory: Successfully created connection to netease/127.0.1.1:7077 after 16 ms (0 ms spent in bootstraps)
18/09/09 13:06:10 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180909130610-0371
18/09/09 13:06:10 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180909130610-0371/0 on worker-20180905101745-10.240.193.109-34417 (10.240.193.109:34417) with 6 cores
18/09/09 13:06:10 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180909130610-0371/0 on hostPort 10.240.193.109:34417 with 6 cores, 1024.0 MB RAM
18/09/09 13:06:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44721.
18/09/09 13:06:10 INFO netty.NettyBlockTransferService: Server created on 10.240.193.109:44721
18/09/09 13:06:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/09/09 13:06:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.193.109, 44721, None)
18/09/09 13:06:10 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180909130610-0371/0 is now RUNNING
18/09/09 13:06:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.240.193.109:44721 with 366.3 MB RAM, BlockManagerId(driver, 10.240.193.109, 44721, None)
18/09/09 13:06:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.193.109, 44721, None)
18/09/09 13:06:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.240.193.109, 44721, None)
18/09/09 13:06:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c8a68c1{/metrics/json,null,AVAILABLE,@Spark}
18/09/09 13:06:11 INFO scheduler.EventLoggingListener: Logging events to file:/root/todo/spark_eventlogs/app-20180909130610-0371
18/09/09 13:06:11 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/09/09 13:06:11 INFO internal.SharedState: Warehouse path is 'file:/home/netease/test-for-spark/spark-warehouse/'.
18/09/09 13:06:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c82cd4f{/SQL,null,AVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@289fdb08{/SQL/json,null,AVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b61a019{/SQL/execution,null,AVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ce9e05a{/SQL/execution/json,null,AVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53ac845a{/static/sql,null,AVAILABLE,@Spark}
Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://netease:9000/user/root/DFSReadWriteTest;
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:382)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:370)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:506)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:486)
	at org.apache.spark.examples.HdfsTest$.main(HdfsTest.scala:36)
	at org.apache.spark.examples.HdfsTest.main(HdfsTest.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/09/09 13:06:11 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/09/09 13:06:11 INFO server.ServerConnector: Stopped Spark@3dd69f5a{HTTP/1.1}{0.0.0.0:4040}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25f9407e{/stages/stage/kill,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@21ec5d87{/jobs/job/kill,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1eba372c{/api,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f2bf0e2{/,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3bd7f8dc{/static,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6cea706c{/NetEase Executors/ne-threadDump/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@476aac9{/NetEase Executors/ne-threadDump,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1b11ef33{/NetEase Executors/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@54107f42{/NetEase Executors,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@48c40605{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@770d0ea6{/executors/threadDump,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@302fec27{/executors/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@60cf80e7{/executors,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24f360b2{/environment/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a2da905{/environment,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@54709809{/storage/rdd/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@355e34c7{/storage/rdd,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1133ec6e{/storage/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6063d80a{/storage,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ae76500{/stages/pool/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2fd1731c{/stages/pool,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ae81e1{/stages/stage/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@59fc684e{/stages/stage,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@46c670a6{/stages/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@782a4fff{/stages,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@30ed9c6c{/jobs/job/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@53499d85{/jobs/job,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@64712be{/jobs/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@759fad4{/jobs,null,UNAVAILABLE,@Spark}
18/09/09 13:06:11 INFO ui.SparkUI: Stopped Spark web UI at http://10.240.193.109:4040
18/09/09 13:06:11 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/09/09 13:06:11 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/09/09 13:06:11 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/09/09 13:06:11 INFO memory.MemoryStore: MemoryStore cleared
18/09/09 13:06:11 INFO storage.BlockManager: BlockManager stopped
18/09/09 13:06:11 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/09/09 13:06:11 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/09/09 13:06:11 INFO spark.SparkContext: Successfully stopped SparkContext
18/09/09 13:06:11 INFO util.ShutdownHookManager: Shutdown hook called
18/09/09 13:06:11 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-922b9383-569b-42af-841b-4f11fca2f99e
./run-examples-core.sh: line 22: 2: command not found
Usage: LocalALS <M> <U> <F> <iters>
./run-examples-core.sh: line 22: 2: command not found
WARN: This is a naive implementation of Logistic Regression and is given as an example!
Please use org.apache.spark.ml.classification.LogisticRegression
for more conventional use.
      
Exception in thread "main" java.io.FileNotFoundException: hdfs:/netease:9000/user/root/DFSReadWriteTest (No such file or directory)
	at java.io.FileInputStream.open0(Native Method)
	at java.io.FileInputStream.open(FileInputStream.java:195)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at scala.io.Source$.fromFile(Source.scala:91)
	at scala.io.Source$.fromFile(Source.scala:76)
	at scala.io.Source$.fromFile(Source.scala:54)
	at org.apache.spark.examples.LocalFileLR$.main(LocalFileLR.scala:54)
	at org.apache.spark.examples.LocalFileLR.main(LocalFileLR.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
./run-examples-core.sh: line 22: 2: command not found
Usage: SparkHdfsLR <file> <iters>
./run-examples-core.sh: line 22: 2: command not found
Usage: SparkKMeans <file> <k> <convergeDist>
./run-examples-core.sh: line 22: 2: command not found
WARN: This is a naive implementation of PageRank and is given as an example!
Please use the PageRank implementation found in org.apache.spark.graphx.lib.PageRank
for more conventional use.
      
18/09/09 13:06:13 INFO spark.SparkContext: Running Spark version 2.1.2
18/09/09 13:06:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
18/09/09 13:06:14 WARN util.Utils: Your hostname, netease resolves to a loopback address: 127.0.1.1; using 10.240.193.109 instead (on interface enp1s0)
18/09/09 13:06:14 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
18/09/09 13:06:14 INFO spark.SecurityManager: Changing view acls to: root
18/09/09 13:06:14 INFO spark.SecurityManager: Changing modify acls to: root
18/09/09 13:06:14 INFO spark.SecurityManager: Changing view acls groups to: 
18/09/09 13:06:14 INFO spark.SecurityManager: Changing modify acls groups to: 
18/09/09 13:06:14 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
18/09/09 13:06:14 INFO util.Utils: Successfully started service 'sparkDriver' on port 36575.
18/09/09 13:06:14 INFO spark.SparkEnv: Registering MapOutputTracker
18/09/09 13:06:14 INFO spark.SparkEnv: Registering BlockManagerMaster
18/09/09 13:06:14 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
18/09/09 13:06:14 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
18/09/09 13:06:14 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-ae07c274-a85b-4ffe-b648-61b5d8cbe39e
18/09/09 13:06:14 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
18/09/09 13:06:14 INFO spark.SparkEnv: Registering OutputCommitCoordinator
18/09/09 13:06:14 INFO util.log: Logging initialized @993ms
18/09/09 13:06:14 INFO server.Server: jetty-9.2.z-SNAPSHOT
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/jobs,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/jobs/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/jobs/job,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/jobs/job/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/stages,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/stages/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/stages/stage,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/stages/stage/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/stages/pool,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/stages/pool/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/storage,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/storage/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/storage/rdd,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/storage/rdd/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@770d0ea6{/environment,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48c40605{/environment/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54107f42{/executors,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b11ef33{/executors/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@476aac9{/executors/threadDump,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6cea706c{/executors/threadDump/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3bd7f8dc{/NetEase Executors,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f2bf0e2{/NetEase Executors/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1eba372c{/NetEase Executors/ne-threadDump,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21ec5d87{/NetEase Executors/ne-threadDump/json,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25f9407e{/static,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@552518c3{/,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a69561c{/api,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59aa20b3{/jobs/job/kill,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@363f6148{/stages/stage/kill,null,AVAILABLE,@Spark}
18/09/09 13:06:14 INFO server.ServerConnector: Started Spark@5003041b{HTTP/1.1}{0.0.0.0:4040}
18/09/09 13:06:14 INFO server.Server: Started @1069ms
18/09/09 13:06:14 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
18/09/09 13:06:14 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.240.193.109:4040
18/09/09 13:06:14 INFO spark.SparkContext: Added JAR file:/root/todo/spark-1.1.1/examples/jars/scopt_2.11-3.3.0.jar at spark://10.240.193.109:36575/jars/scopt_2.11-3.3.0.jar with timestamp 1536469574664
18/09/09 13:06:14 INFO spark.SparkContext: Added JAR file:/root/todo/spark-1.1.1/examples/jars/spark-examples_2.11-2.1.2.jar at spark://10.240.193.109:36575/jars/spark-examples_2.11-2.1.2.jar with timestamp 1536469574664
18/09/09 13:06:14 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://netease:7077...
18/09/09 13:06:14 INFO client.TransportClientFactory: Successfully created connection to netease/127.0.1.1:7077 after 15 ms (0 ms spent in bootstraps)
18/09/09 13:06:14 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20180909130614-0372
18/09/09 13:06:14 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20180909130614-0372/0 on worker-20180905101745-10.240.193.109-34417 (10.240.193.109:34417) with 6 cores
18/09/09 13:06:14 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20180909130614-0372/0 on hostPort 10.240.193.109:34417 with 6 cores, 1024.0 MB RAM
18/09/09 13:06:14 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39777.
18/09/09 13:06:14 INFO netty.NettyBlockTransferService: Server created on 10.240.193.109:39777
18/09/09 13:06:14 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
18/09/09 13:06:14 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.240.193.109, 39777, None)
18/09/09 13:06:14 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.240.193.109:39777 with 366.3 MB RAM, BlockManagerId(driver, 10.240.193.109, 39777, None)
18/09/09 13:06:14 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20180909130614-0372/0 is now RUNNING
18/09/09 13:06:14 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.240.193.109, 39777, None)
18/09/09 13:06:14 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.240.193.109, 39777, None)
18/09/09 13:06:14 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@607b2792{/metrics/json,null,AVAILABLE,@Spark}
18/09/09 13:06:15 INFO scheduler.EventLoggingListener: Logging events to file:/root/todo/spark_eventlogs/app-20180909130614-0372
18/09/09 13:06:15 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
18/09/09 13:06:15 INFO internal.SharedState: Warehouse path is 'file:/home/netease/test-for-spark/spark-warehouse/'.
18/09/09 13:06:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30814f43{/SQL,null,AVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49cf9028{/SQL/json,null,AVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fd5717c{/SQL/execution,null,AVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d774249{/SQL/execution/json,null,AVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@344426bf{/static/sql,null,AVAILABLE,@Spark}
Exception in thread "main" org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://netease:9000/user/root/DFSReadWriteTest;
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:382)
	at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:370)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)
	at org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:506)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:542)
	at org.apache.spark.sql.DataFrameReader.textFile(DataFrameReader.scala:515)
	at org.apache.spark.examples.SparkPageRank$.main(SparkPageRank.scala:64)
	at org.apache.spark.examples.SparkPageRank.main(SparkPageRank.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
18/09/09 13:06:15 INFO spark.SparkContext: Invoking stop() from shutdown hook
18/09/09 13:06:15 INFO server.ServerConnector: Stopped Spark@5003041b{HTTP/1.1}{0.0.0.0:4040}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@363f6148{/stages/stage/kill,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@59aa20b3{/jobs/job/kill,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1a69561c{/api,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@552518c3{/,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@25f9407e{/static,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@21ec5d87{/NetEase Executors/ne-threadDump/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1eba372c{/NetEase Executors/ne-threadDump,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2f2bf0e2{/NetEase Executors/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3bd7f8dc{/NetEase Executors,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6cea706c{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@476aac9{/executors/threadDump,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1b11ef33{/executors/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@54107f42{/executors,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@48c40605{/environment/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@770d0ea6{/environment,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@302fec27{/storage/rdd/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@60cf80e7{/storage/rdd,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@24f360b2{/storage/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2a2da905{/storage,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@54709809{/stages/pool/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@355e34c7{/stages/pool,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1133ec6e{/stages/stage/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@6063d80a{/stages/stage,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ae76500{/stages/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2fd1731c{/stages,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@5ae81e1{/jobs/job/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@59fc684e{/jobs/job,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@46c670a6{/jobs/json,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@782a4fff{/jobs,null,UNAVAILABLE,@Spark}
18/09/09 13:06:15 INFO ui.SparkUI: Stopped Spark web UI at http://10.240.193.109:4040
18/09/09 13:06:15 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
18/09/09 13:06:15 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
18/09/09 13:06:15 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
18/09/09 13:06:15 INFO memory.MemoryStore: MemoryStore cleared
18/09/09 13:06:15 INFO storage.BlockManager: BlockManager stopped
18/09/09 13:06:15 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
18/09/09 13:06:15 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
18/09/09 13:06:15 INFO spark.SparkContext: Successfully stopped SparkContext
18/09/09 13:06:15 INFO util.ShutdownHookManager: Shutdown hook called
18/09/09 13:06:15 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-c487d280-c7e7-4f8b-9eab-c9e996dccaef
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/DFSReadWriteTest': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/myModelPath': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `hdfs://netease:9000/user/root/target': No such file or directory
rmr: DEPRECATED: Please use 'rm -r' instead.
rmr: `/tmp/kmeans': No such file or directory
